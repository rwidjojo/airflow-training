# LocalExecutor Installation

[<img src="../1. Airflow Fundamentals/imgs/2-architecture/architecture-basic.jpg">](https://airflow.apache.org/docs/apache-airflow/stable/executor/local.html)

LocalExecutor we are basically running all Airflow components from the same physical environment or simply in one VM.

In one machine there will be multiple OS processes running the Web Server, Scheduler and Workers.

## Direct Installation

Directly install airflow to your local machine

### Requirement

- Ubuntu 18.04 (or Debian based linux distro)
- Python 3.6

### Postgresql Setup

To simplify setup process we will also use same machine to setup postgresql. But you could also use managed posgtres service such as AWS RDS for Posgtresql, GCP Cloud SQL, or Azure Database.

To install PostgreSql we can simply run the following in command:

```bash
sudo apt-get install postgresql postgresql-contrib
```

In a few minutes, PostgreSql should be installed.

Next, we need to set it up. First step is creating a psql object:

```bash
sudo -u postgres psql
```

It will bring you psql console, and your terminal will show it like this

```bash
postgres=#
```

Create username `airflow` with password `airflow` (feel free to change this)

```sql
CREATE USER airflow PASSWORD 'airflow';
```

Then create database `airflow` to place airflow metadata. You could change database name with anything you want

```sql
CREATE DATABASE airflow; # you could change this db name
```

Then grant access user `airflow` to public schema db `airflow`

```sql
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO airflow;
```

To exit from `psql` console you could type `\q` and press `Enter`

```bash
postgres=# \q
```

**NOTES: When using `psql` console dont forget add semicolon (`;`) at the end every statement**

Finally, we need to install libpq-dev for enabling us to implement a PostgreSql client:

```bash
sudo apt install libpq-dev
```

### Python 3

Before installing airflow, make sure you already has python3 installed in your machine. Here is [python3 installation guide](https://www.python.org/downloads/)

Verify your python installation

```bash
python3 --version
```

make sure output is `Python 3.6.9` or version greater

Because we wil using pip as package manager to install airflow, verify your pip installation

```bash
pip3 --version
```

or

```bash
pip --version
```

if either command above is not available then install pip yourself with [this guide](https://pip.pypa.io/en/stable/installing/)

### Install Airflow

Set environment variable to set where Airflow home directory will be placed

In unix based system (Linux/Mac) we could use this command

```bash
export AIRFLOW_HOME=~/airflow
```

In windows use this command

```shell
set export AIRFLOW_HOME=[DIRECTORY]/airflow
```

change `[DIRECTORY]` to directory path where you want to place Airflow home directory

Install Airflow 1.10.14

```bash
pip install apache-airflow==1.10.14
```

Because we use postgresql as metadata database then we need to install airflow postgresql package and postgresql client

```bash
pip install apache-airflow['postgresql']
psycopg2
```

and because we will use EMR, S3 and Redshift we need install airflow aws package

```bash
pip install 'apache-airflow[amazon]'
```

verify your installation by command

```bash
airflow version
```

### Configure Airflow

Goto airflow home directory in mycase located in `~/airflow` then edit file `airflow.cfg`

To be able to connect to postgresql, find key `sql_alchemy_conn` and change it value to `postgresql+psycopg2://[USERNAME]:[PASSWORD]@localhost/[DBNAME]` change `[USERNAME]`, `[PASSWORD]` and `[DBNAME]` to your config for example:

```bash
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost/airflow
```

Then change airflow executor, search `executor = SequentialExecutor` and change `SequentialExecutor` to `LocalExecutor` for example

```bash
executor = LocalExecutor
```

Then populate postgresql with Airflow metadata by command

```bash
airflow initdb
```

### Starting Airflow

As mention in image above **LocalExecutor** only using 2 services webserver and scheduler so we need to start that 2 services

```bash
airflow webserver -D
airflow scheduler -D
```

open on your browser [`localhost:8080`](http://localhost:8080) (localhost could be replaced with you machine IP)
