# LocalExecutor Installation

[<img src="../1. Airflow Fundamentals/imgs/2-architecture/architecture-basic.jpg">](https://airflow.apache.org/docs/apache-airflow/stable/executor/local.html)

LocalExecutor we are basically running all Airflow components from the same physical environment or simply in one VM.

In one machine there will be multiple OS processes running the Web Server, Scheduler and Workers.

## Direct Installation

Directly install airflow to your local machine

### Requirement

- Ubuntu 18.04 (or Debian based linux distro)
- Python 3.6

### Postgresql Setup

To simplify setup process we will also use same machine to setup postgresql. But you could also use managed posgtres service such as AWS RDS for Posgtresql, GCP Cloud SQL, or Azure Database.

To install PostgreSql we can simply run the following in command:

```bash
sudo apt-get install postgresql postgresql-contrib
```

In a few minutes, PostgreSql should be installed.

Next, we need to set it up. First step is creating a psql object:

```bash
sudo -u postgres psql
```

It will bring you psql console, and your terminal will show it like this

```bash
postgres=#
```

Create username `airflow` with password `airflow` (feel free to change this)

```sql
CREATE USER airflow PASSWORD 'airflow';
```

Then create database `airflow` to place airflow metadata. You could change database name with anything you want

```sql
CREATE DATABASE airflow; # you could change this db name
```

Then grant access user `airflow` to public schema db `airflow`

```sql
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO airflow;
```

To exit from `psql` console you could type `\q` and press `Enter`

```bash
postgres=# \q
```

**NOTES: When using `psql` console dont forget add semicolon (`;`) at the end every statement**

Finally, we need to install libpq-dev for enabling us to implement a PostgreSql client:

```bash
sudo apt install libpq-dev
```

### Python 3

Before installing airflow, make sure you already has python3 installed in your machine. Here is [python3 installation guide](https://www.python.org/downloads/)

Verify your python installation

```bash
python3 --version
```

make sure output is `Python 3.6.9` or version greater

Because we wil using pip as package manager to install airflow, verify your pip installation

```bash
pip3 --version
```

or

```bash
pip --version
```

if either command above is not available then install pip yourself with [this guide](https://pip.pypa.io/en/stable/installing/)

### Install Airflow

Set environment variable to set where Airflow home directory will be placed

In unix based system (Linux/Mac) we could use this command

```bash
export AIRFLOW_HOME=~/airflow
```

In windows use this command

```shell
set export AIRFLOW_HOME=[DIRECTORY]/airflow
```

change `[DIRECTORY]` to directory path where you want to place Airflow home directory

Install Airflow 1.10.14

```bash
pip install apache-airflow==1.10.14
```

Because we use postgresql as metadata database then we need to install airflow postgresql package and postgresql client

```bash
pip install apache-airflow['postgresql']
psycopg2
```

and because we will use EMR, S3 and Redshift we need install airflow aws package

```bash
pip install 'apache-airflow[amazon]'
```

verify your installation by command

```bash
airflow version
```

### Configure Airflow

Goto airflow home directory in mycase located in `~/airflow` then edit file `airflow.cfg`

To be able to connect to postgresql, find key `sql_alchemy_conn` and change it value to `postgresql+psycopg2://[USERNAME]:[PASSWORD]@localhost/[DBNAME]` change `[USERNAME]`, `[PASSWORD]` and `[DBNAME]` to your config for example:

```bash
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost/airflow
```

Then change airflow executor, search `executor = SequentialExecutor` and change `SequentialExecutor` to `LocalExecutor` for example

```bash
executor = LocalExecutor
```

Save and close file `airflow.cfg`

Then populate postgresql with Airflow metadata by command

```bash
airflow initdb
```

### Starting Airflow

As mention in image above **LocalExecutor** only using 2 services webserver and scheduler so we need to start that 2 services

```bash
airflow webserver -D
airflow scheduler -D
```

open on your browser [`localhost:8080`](http://localhost:8080) (localhost could be replaced with you machine IP)

## Docker Installation

Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications.

With docker you will have no worries software installation will destroy your local environment. The most important thing is when you use docker to build software then that software will be able to run anyware.

### Requirement

- Docker ([follow this setup guide](https://docs.docker.com/get-docker/))
- Docker Compose ([follow this setup guide](https://docs.docker.com/compose/install/))

### Docker Image

We will use image from [puckel/docker-airflow](https://hub.docker.com/r/puckel/docker-airflow) because in version 1.10 docker image in official Airflow repo is not stable yet and this is replacement that widely accepted and used by community.

### Docker Compose

Docker compose Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your applicationâ€™s services. Then, with a single command, you create and start all the services from your configuration. To learn more about all the features of Compose, see the list of features.

First we need to create directory for docker compose yaml

```bash
mkdir airflow-docker
cd airflow-docker
```

create folder `dags` with command

```bash
mkdir dags
```

create file `LocalExecutor.yaml` and copy this yaml

```yaml
version: '3.7'
services:
  postgres:
    image: postgres:9.6
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    logging:
      options:
        max-size: 10m
        max-file: '3'

  webserver:
    image: puckel/docker-airflow:1.10.9
    restart: always
    depends_on:
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
    logging:
      options:
        max-size: 10m
        max-file: '3'
    volumes:
      - ./dags:/usr/local/airflow/dags
      # - ./plugins:/usr/local/airflow/plugins
    ports:
      - '8080:8080'
    command: webserver
    healthcheck:
      test: ['CMD-SHELL', '[ -f /usr/local/airflow/airflow-webserver.pid ]']
      interval: 30s
      timeout: 30s
      retries: 3
```

YAML file above means we will have 2 services

- `postgres` service will use docker image [`postgres:9.6`](https://hub.docker.com/layers/postgres/library/postgres/9.6/images/sha256-7b13cde8a008196f254b751d8b572f58937acd5864a2fa467b36d270fa76f747?context=explore) as name dicate will staring postgresql version 9.6
- `webserver` service will use docker image [`puckel/docker-airflow:1.10.9`](<(https://hub.docker.com/r/puckel/docker-airflow)>) and as name dicates it will using airflow version 1.10.9

#### Lets review `postgresql` service configuration

```yaml
environment:
  - POSTGRES_USER=airflow
  - POSTGRES_PASSWORD=airflow
  - POSTGRES_DB=airflow
```

Configuration above is to configure username, password and database that will be created in postgresql.

#### Continue with `webserver` service configuration:

```yaml
depends_on:
  - postgres
```

This config state that service **webserver** will depends on service **postgres** so webserver service will only start if postgres service is already running and healthy.

```yaml
environment:
  - LOAD_EX=n
  - EXECUTOR=Local
```

Configuration above is to configure webserver environment.

- `LOAD_EX=n` means that it will not load airflow official examples. If you want to load examples then change `n` to `y` become `LOAD_EX=y`
- `EXECUTOR=Local` is to configure that airflow image will use LocalExecutor and under the hood will start webserver and scheduler in the same image container.

```yaml
volumes:
  - ./dags:/usr/local/airflow/dags
  # - ./plugins:/usr/local/airflow/plugins
```

Configuration above is to mount your local directory to image container directory or in the simple way it will link your local directory to image container directory

- `./dags:/usr/local/airflow/dags` means that your local `dags` directory will be mounted to `/usr/local/airflow/dags` image container directory. So to create dag file you just create it in directory `airflow-docker/dags`.

- `./plugins:/usr/local/airflow/plugins` means that your local `plugins` directory will be mounted to `/usr/local/airflow/plugins` image container directory. So to create plugin file you just create it in directory `airflow-docker/plugins`. But because we will not use plugin yet we will comment this config first.

```yaml
ports:
  - '8080:8080'
```

Configuration above is to configure port fowarding from image container to your local machine. It means port `8080` in image container will be fowared to your local port `8080`.

### How to run

To run docker compose above then run this command:

```bash
docker-compose -f LocalExecutor.yaml up -d
```

it will starting your service and open on your browser [`localhost:8080`](http://localhost:8080) (localhost could be replaced with you machine IP)
