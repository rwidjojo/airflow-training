# 17. Incremental Data Processing

Whenever working with any sort of data, these are essential details to know. Any data, both small and big, can be complex and it is important to have a technical plan of approach before building a pipeline.

The solution is always dependant on what you, or other users, want
to do with the data so ask yourself and others questions such as **“Do we want to process the data again at some other time in the future?”**, **”How do I receive the data (i.e., frequency,
size, format, source type)?”**, and **“What are we going to build with the data?”**

After knowing the answers to such questions, we can think of the technical details.

Let’s download one single hourly dump and inspect the data by hand. In order to develop a data pipeline, we must understand how to load it in an incremental fashion and how to work the data.

## 17.1. Scenario

We have table `recent_orders` that orders record on dec 2020. Here is the table structure:

```sql
CREATE TABLE recent_orders (
    order_id smallint NOT NULL PRIMARY KEY,
    customer_id bpchar,
    employee_id smallint,
    order_date date,
    required_date date,
    shipped_date date,
    ship_via smallint,
    freight real,
    ship_name character varying(40),
    ship_address character varying(60),
    ship_city character varying(15),
    ship_region character varying(15),
    ship_postal_code character varying(10),
    ship_country character varying(15)
);
```

You as data enginee want to count data from table `recent_orders` daily grouped by `ship_country` and ingest it to your aggregate table.

## 17.2 SQL Script

```sql
SELECT order_date, ship_country, COUNT(order_id)
FROM public.recent_orders
WHERE order_date = '2020-12-01' # because daily incremental then this value need changed to current execution date
GROUP BY order_date, ship_country
ORDER BY order_date
```

## 17.3 Python Operator

To read data you could use function `read_data` from `lesson3.exercise1.py`

```python
def read_data():
  db_conn = PostgresHook(postgres_conn_id='john_doe_postgres')
  result = db_conn.get_records("""
  SELECT order_date, ship_country, COUNT(order_id)
  FROM public.recent_orders
  WHERE order_date = '2020-12-01'
  GROUP BY order_date, ship_country
  ORDER BY order_date
  """)
  for row in result:
    logging.info(row)
```

But if you look code above, it use static date filter on `WHERE order_date = '2020-12-01'`

so you need to replace it with airflow context `ds`. Add option `provide_context=True` to the task

```python
read_task = PythonOperator(
    task_id="read",
    python_callable=read_data,
    provide_context=True,
    dag=dag,
)
```

after that capture argument `ds` on function `read_data` and replace date `2020-12-01` with `ds`

```python
def read_data(ds, **kwargs):
  db_conn = PostgresHook(postgres_conn_id='john_doe_postgres')

  # format sql string so can accept `ds`
  sql = f"""
  SELECT order_date, ship_country, COUNT(order_id) ship_count
  FROM public.recent_orders
  WHERE order_date = '{ds}'
  GROUP BY order_date, ship_country
  ORDER BY order_date
  """

  # log formatted sql so it easier to debug
  loggin.info(sql)

  # execute sql
  result = db_conn.get_records(sql)

  # log each row result
  for row in result:
    logging.info(row)
```

## 17.4 Save Aggregate

After you get count data then you need to save it to aggregate table `shipping_count` that has schema:

```sql
CREATE TABLE shipping_count (
    order_date date NOT NULL,
    ship_country character varying(15),
    ship_count int,
    PRIMARY KEY (order_date, ship_country)
);
```

First ou need to declare INSERT statement

```sql
INSERT INTO shipping_count(order_date, ship_country, ship_count)
VALUES(%s, %s, %s)
ON CONFLICT (order_date, ship_country)
DO NOTHING;
```

then iterate each row from result to insert with INSERT statement above:

```python
insert_sql = """
INSERT INTO public.shipping_count(order_date, ship_country, ship_count)
VALUES(%s, %s, %s)
ON CONFLICT (order_date, ship_country)
DO NOTHING;
"""
for row in result:
    logging.info(insert_sql)
    dest_conn.run(
      insert_sql,
      parameters=(
        row['order_date'],
        row['ship_country'],
        row['ship_count'])
      )
```
