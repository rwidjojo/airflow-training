# Write Logs to Remote Locations (S3)

Airflow able to store execution logs to remote location for example AWS S3

## Create Connection

First we need create S3 connection so Airflow could communicate with it.

```bash
airflow connections --add --conn_id aws_conn_s3 --conn_type s3 --conn_extra "{\"aws_access_key_id\":\"[AWS_ACCESS_KEY]\", \"aws_secret_access_key\": \"[AWS_SECRET_KEY]\"}"
```

replace `[AWS_ACCESS_KEY]` and `[AWS_SECRET_KEY]` with your access and secret key

for example:

```bash
airflow connections --add --conn_id aws_conn_s3 --conn_type s3 --conn_extra "{\"aws_access_key_id\":\"fISfjoiKSJSLjosl\", \"aws_secret_access_key\": \"vasdjflasjdflajsdlfjI23sfjs\"}"
```

verify your newly created connection by command

```bash
airflow connections --list
```

## Configuration

To enable remote loggin then edit `airflow.cfg` find `[core]` section and update this config

```bash
remote_logging=True
# example: s3://pmi_bucket/airflow-logs/
remote_base_log_folder=s3://YOUR_BUCKET/FOLDER
remote_log_conn_id=aws_conn_s3 # use connection id that you created previously
encrypt_s3_logs=False
```

or if you want use environment variables:

```bash
export AIRFLOW__CORE__REMOTE_LOGGING=True,
export AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER='s3://kata-data-bucket/airflow/logs',
export AIRFLOW__CORE__REMOTE_LOG_CONN_ID='aws_conn_s3',
export AIRFLOW__CORE__ENCRYPT_S3_LOGS='False',
```

Finally start your airflow service, or if your airflow already running then restart all service.
