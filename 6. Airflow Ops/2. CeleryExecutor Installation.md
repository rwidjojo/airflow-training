# CeleryExecutor Installation

[<img src="../1. Airflow Fundamentals/imgs/2-architecture/architecture-celery.jpg">](https://airflow.apache.org/docs/apache-airflow/stable/executor/local.html)

## Direct Installation

### Requirement

- Same as [LocalExecutor](1.%20LocalExecutor%20Installation.md)
- [Redis Installation](https://redis.io/download)

### Airflow Configuration

You could use previous Airflow LocalExecutor installation but first you need to stop all airflow running service first.

Open file `airflow.cfg`

search text `broker_url` and change its value to `redis://:[REDIS_PASSWORD]@[REDIS_HOST]:[REDIS_PORT]/[REDIS_DBNUM]` if redis has password or `redis://[REDIS_HOST]:[REDIS_PORT]/[REDIS_DBNUM]` if redis does not has password

For example:

```bash
redis://localhost:6379/1
```

Then search text `result_backend` and change its value to same with value `sql_alchemy_conn`

For example:

```bash
result_backend = postgresql+psycopg2://airflow:airflow@localhost/airflow
```

Then search text `store_serialized_dags` and change its value to `True`
and also add config `store_dag_code = True`

For example:

```bash
store_serialized_dags = True
store_dag_code = True
```

This config will enable [Airflow DAG serialization](https://airflow.apache.org/docs/apache-airflow/1.10.14/dag-serialization.html#enable-dag-serialization) so webserver and worker will read DAG files from database instead actual files.

Finally change `executor` to `CeleryExecutor`

For example:

```bash
executor = CeleryExecutor
```

Save and close file `airflow.cfg`

Because default airflow installation does not include celery pacakge then we need install it first

```bash
pip install 'apache-airflow[celery]'
```

### Starting Airflow

First we need to start scheduler and webserver

```bash
airflow webserver -D
airflow scheduler -D
```

Then start apache flower that manage celery in airflow

```bash
airflow flower
```

And finally start worker

```bash
airflow worker
```

### Multiple Machines

As stated before, we could add new worker on new machine. In principle each airflow service could be started on different machine instance.

Things that needed to prepare is:

- Make sure that postgresql and redis could be accessed from each machine.
- File `airflow.cfg` must have same value in each machine. So if you need to change airflow config you must also change config in other machines.
- If DAG serialization enabled then place DAG files to machine where `scheduler` located
- If DAG serialization is not enabled then each machine must be able access the same DAG files
  - This could be handled by NFS system or if in AWS then Amazon EFS
  - or could use S3
  - or could use git sync

Here how each airflow service communicate
The components communicate with each other in many places

- [1] Web server –> Workers - Fetches task execution logs
- [2] Web server –> DAG files - Reveal the DAG structure (if serialization enabled then will read to DB)
- [3] Web server –> Database - Fetch the status of the tasks
- [4] Workers –> DAG files - Reveal the DAG structure and execute the tasks (if serialization enabled then will read to DB)
- [5] Workers –> Database - Gets and stores information about connection configuration, variables and XCOM.
- [6] Workers –> Celery’s result backend - Saves the status of tasks
- [7] Workers –> Celery’s broker (Redis) - Stores commands for execution
- [8] Scheduler –> Database - Store a DAG run and related tasks
- [9] Scheduler –> DAG files - Reveal the DAG structure and execute the tasks
- [10] Scheduler –> Celery’s result backend - Gets information about the status of completed tasks
- [11] Scheduler –> Celery’s broker (Redis) - Put the commands to be executed

## Docker

You could use `airflow-docker` folder and create file `CeleryExecutor.yaml` and copy this yaml

```yaml
version: '2.1'
services:
  redis:
    image: 'redis:5.0.5'
    # command: redis-server --requirepass redispass

  postgres:
    image: postgres:9.6
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    # Uncomment these lines to persist data on the local filesystem.
    #     - PGDATA=/var/lib/postgresql/data/pgdata
    # volumes:
    #     - ./pgdata:/var/lib/postgresql/data/pgdata

  webserver:
    image: puckel/docker-airflow:1.10.9
    restart: always
    depends_on:
      - postgres
      - redis
    environment:
      - LOAD_EX=n
      - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - EXECUTOR=Celery
      # - POSTGRES_USER=airflow
      # - POSTGRES_PASSWORD=airflow
      # - POSTGRES_DB=airflow
      # - REDIS_PASSWORD=redispass
    volumes:
      - ./dags:/usr/local/airflow/dags
      # Uncomment to include custom plugins
      # - ./plugins:/usr/local/airflow/plugins
    ports:
      - '8080:8080'
    command: webserver
    healthcheck:
      test: ['CMD-SHELL', '[ -f /usr/local/airflow/airflow-webserver.pid ]']
      interval: 30s
      timeout: 30s
      retries: 3

  flower:
    image: puckel/docker-airflow:1.10.9
    restart: always
    depends_on:
      - redis
    environment:
      - EXECUTOR=Celery
      # - REDIS_PASSWORD=redispass
    ports:
      - '5555:5555'
    command: flower

  scheduler:
    image: puckel/docker-airflow:1.10.9
    restart: always
    depends_on:
      - webserver
    volumes:
      - ./dags:/usr/local/airflow/dags
      # Uncomment to include custom plugins
      # - ./plugins:/usr/local/airflow/plugins
    environment:
      - LOAD_EX=n
      - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - EXECUTOR=Celery
      # - POSTGRES_USER=airflow
      # - POSTGRES_PASSWORD=airflow
      # - POSTGRES_DB=airflow
      # - REDIS_PASSWORD=redispass
    command: scheduler

  worker:
    image: puckel/docker-airflow:1.10.9
    restart: always
    depends_on:
      - scheduler
    volumes:
      - ./dags:/usr/local/airflow/dags
      # Uncomment to include custom plugins
      # - ./plugins:/usr/local/airflow/plugins
    environment:
      - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - EXECUTOR=Celery
      # - POSTGRES_USER=airflow
      # - POSTGRES_PASSWORD=airflow
      # - POSTGRES_DB=airflow
      # - REDIS_PASSWORD=redispass
    command: worker
```

YAML file above means we will have 6 services

- `redis` service will use docker image [`redis:5.0.5`](https://hub.docker.com/layers/redis/library/redis/5.0.5-buster/images/sha256-e93374903e0af2ee9fb9bbf2fe337c6ccf1ed6f3fee62554a0ac2c86a71600a7?context=explore) as name dictate it will start redis version 5.0.5
- `postgres` service will use docker image [`postgres:9.6`](https://hub.docker.com/layers/postgres/library/postgres/9.6/images/sha256-7b13cde8a008196f254b751d8b572f58937acd5864a2fa467b36d270fa76f747?context=explore) as name dicate it will staring postgresql version 9.6
- `webserver`, `scheduler`, `flower` and `worker` service will use docker image [`puckel/docker-airflow:1.10.9`](<(https://hub.docker.com/r/puckel/docker-airflow)>) and as name dicates it will using airflow version 1.10.9

Explanation about service `webserver`, `scheduler`, `flower` and `worker` is same with LocalExecutor the difference is

- Executor change to => `EXECUTOR=Celery`
- `webserver` and `scheduler` now is separated service.
- `flower` service is manage airflow celery
- `woker` service is start airflow worker
- New environment variables `FERNET_KEY` to make sure all service use same fernet key.
  - As stated before that `airflow.cfg` need to exacty same on all services.
  - Other configuration is already handled by docker. And only this fernet key that needed to configured manually.
  - Fernet key will be used to encrypt and decrypt sensitive data in Airflow.

### How to run

To run docker compose above then run this command:

```bash
docker-compose -f CeleryExecutor.yaml up -d
```

it will starting your service and open on your browser [`localhost:8080`](http://localhost:8080) (localhost could be replaced with you machine IP)

### Scale Worker

Easy scaling using docker-compose:

```bash
docker-compose -f docker-compose-CeleryExecutor.yml scale worker=3
```
