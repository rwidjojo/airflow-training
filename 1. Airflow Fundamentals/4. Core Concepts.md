# 4. Core Concepts

## 4.1 DAGs

In Airflow, a `DAG` -- or a Directed Acyclic Graph -- is a collection of all
the tasks you want to run, organized in a way that reflects their relationships
and dependencies.

A DAG is defined in a Python script, which represents the DAGs structure (tasks
and their dependencies) as code.

For example, a simple DAG could consist of three tasks: A, B, and C. It could
say that A has to run successfully before B can run, but C can run anytime. It
could say that task A times out after 5 minutes, and B can be restarted up to 5
times in case it fails. It might also say that the workflow will run every night
at 10pm, but shouldn't start until a certain date.

A DAG describes _how_ you want to carry out your workflow; The important thing is that the DAG isn't
concerned with what its tasks do; its job is to make sure that
whatever they do happens at the right time, or in the right order, or with the
right handling of any unexpected issues.

DAGs are defined in standard Python files that are placed in Airflow's
`DAG_FOLDER`. Airflow will execute the code in each file to dynamically build
the `DAG` objects. You can have as many DAGs as you want, each describing an
arbitrary number of tasks. In general, each one should correspond to a single
logical workflow.

<img src="https://airflow.apache.org/docs/apache-airflow/1.10.9/_images/dags.png">

DAG must appear in globals(). Consider the following two DAGs. Only dag_1 will be loaded; the other one only appears in a local scope.

```python
dag_1 = DAG('this_dag_will_be_discovered')

def my_function():
    dag_2 = DAG('but_this_dag_will_not')

my_function()
```

## 4.2 Operator

While DAGs describe how to run a workflow, Operators determine what actually gets done by a task.

An operator describes a single task in a workflow. Operators are usually (but not always) atomic, meaning they can stand on their own and donâ€™t need to share resources with any other operators. The DAG will make sure that operators run in the correct order; other than those dependencies, operators generally run independently. In fact, they may run on two completely different machines.

Airflow provides operators for many common tasks, including:

- `airflow.operators.bash_operator.BashOperator` - executes a bash command
- `airflow.operators.python_operator.PythonOperator` - calls an arbitrary Python function
- `airflow.operators.email_operator.EmailOperator` - sends an email
- `airflow.operators.http_operator.SimpleHttpOperator` - sends an HTTP request
- `airflow.operators.mysql_operator.MySqlOperator`,
  `airflow.operators.sqlite_operator.SqliteOperator`,
  `airflow.operators.postgres_operator.PostgresOperator`,
  `airflow.operators.mssql_operator.MsSqlOperator`,
  `airflow.operators.oracle_operator.OracleOperator`,
  `airflow.operators.jdbc_operator.JdbcOperator`, etc. - executes a SQL command
- `Sensor` - an Operator that waits (polls) for a certain time, file, database row, S3 key, etc...

## 4.3 Task

A Task defines a unit of work within a DAG; it is represented as a node in the DAG graph, and it is written in Python.

Each task is an implementation of an Operator, for example a `PythonOperator` to execute some Python code,
or a `BashOperator` to run a Bash command.

The task implements an operator by defining specific values for that operator,
such as a Python callable in the case of `PythonOperator` or a Bash command in the case of `BashOperator`.

<img src="https://airflow.apache.org/docs/apache-airflow/1.10.9/_images/graph.png">

### 4.4 Task Lifecycle

A task goes through various stages from start to completion. In the Airflow UI (graph and tree views), these stages are displayed by a color representing each stage:

<img src="https://airflow.apache.org/docs/apache-airflow/1.10.9/_images/task_stages.png">

The complete lifecycle of the task looks like this:

<img src="https://airflow.apache.org/docs/apache-airflow/1.10.9/_images/task_lifecycle_diagram.png">

## 4.5 DAG, Task and Operator

<img src="https://drek4537l1klr.cloudfront.net/harenslak/v-5/Figures/image020.png">

## 4.6 Workflows

You're now familiar with the core building blocks of Airflow.
Some of the concepts may sound very similar, but the vocabulary can
be conceptualized like this:

- DAG: The work (tasks), and the order in which
  work should take place (dependencies), written in Python.
- DAG Run: An instance of a DAG for a particular logical date and time.
- Operator: A class that acts as a template for carrying out some work.
- Task: Defines work by implementing an operator, written in Python.
- Task Instance: An instance of a task - that has been assigned to a DAG and has a
  state associated with a specific DAG run (i.e for a specific execution_date).
- execution_date: The logical date and time for a DAG Run and its Task Instances.

By combining `DAGs` and `Operators` to create `TaskInstances`, you can
build complex workflows.
