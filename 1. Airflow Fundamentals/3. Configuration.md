# 3. Setup & Configuration

## 3.1 Installation

Here is overview how to install airflow. For complete instructions please go to [official documentation](https://airflow.apache.org/docs/apache-airflow/stable/installation.html)

### 3.1.1 Installation Sripts

```bash
AIRFLOW_VERSION=1.10.9
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
pip install "apache-airflow[postgres]==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
pip install psycopg2
```

install postgresql

```bash
sudo apt-get install postgresql postgresql-contrib
```

Next, we need to set it up. First step is creating a psql object:

```bash
sudo -u postgres psql
```

We proceed to setting up the required user, database and permissions:

```sql
CREATE USER airflow PASSWORD 'airflow'; #you might wanna change this
CREATE DATABASE airflow;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO airflow;
GRANT
```

Finally, we need to install libpq-dev for enabling us to implement a PostgreSql client:

```bash
sudo apt install libpq-dev
```

## 3.1.2 Extra Packages

The apache-airflow PyPI basic package only installs what’s needed to get started. Subpackages can be installed depending on what will be useful in your environment. For instance, if you don’t need connectivity with Postgres, you won’t have to go through the trouble of installing the postgres-devel yum package, or whatever equivalent applies on the distribution you are using.

Behind the scenes, Airflow does conditional imports of operators that require these extra dependencies

[Full complete list extra packages](https://airflow.apache.org/docs/apache-airflow/stable/installation.html#extra-packages)

## 3.2 Configuration

To configure airflow open file `airflow.cfg` in airflow home directory

For starting point configuration that need to be known is

```bash
# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost/airflow
```

executor we use here is `LocalExecutor` and we use local postgresql database

## 3.3 Starting Airflow

Set Airflow home directory

```bash
export AIRFLOW_HOME=~/airflow
```

For first time start up run db initialization first

```bash
airflow initdb
```

then start webserver and scheduler

```bash
# start the web server, default port is 8080
airflow webserver -p 8080

# start the scheduler
airflow scheduler
```
