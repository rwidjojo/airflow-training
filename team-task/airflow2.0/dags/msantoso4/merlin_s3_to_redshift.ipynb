{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Launch an AWS Redshift Cluster\n",
    "\n",
    "### Step 1: Create an IAM user\n",
    "In most cases, people are given an IAM user to work with. If you are doing this from scratch, then go into your AWS console and create a new IAM user.\n",
    "\n",
    "- Create a new IAM user in your AWS account\n",
    "- Give it `AdministratorAccess`, From `Attach existing policies directly` Tab\n",
    "- Take note of the access key and secret \n",
    "- Edit the file `dwh-iac.cfg` in the same folder as this notebook\n",
    "<font color='red'>\n",
    "<BR>\n",
    "[AWS]<BR>\n",
    "KEY= YOUR_AWS_KEY<BR>\n",
    "SECRET= YOUR_AWS_SECRET<BR>\n",
    "<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (1.1.5)\nRequirement already satisfied: boto3 in /home/ubuntu/.local/lib/python3.8/site-packages (1.16.42)\nRequirement already satisfied: numpy>=1.15.4 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas) (1.19.4)\nRequirement already satisfied: python-dateutil>=2.7.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas) (2020.4)\nRequirement already satisfied: botocore<1.20.0,>=1.19.42 in /home/ubuntu/.local/lib/python3.8/site-packages (from boto3) (1.19.42)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from boto3) (0.3.3)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from boto3) (0.10.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\nRequirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /usr/lib/python3/dist-packages (from botocore<1.20.0,>=1.19.42->boto3) (1.25.8)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Use configparser to read in the variables\n",
    "In the `dwh.cfg` file, you will store your secrets and configuration files. This should not be checked into git.\n",
    "\n",
    "The file looks like this:\n",
    "```\n",
    "[AWS]\n",
    "KEY=<your-key>\n",
    "SECRET=<your-secret>\n",
    "\n",
    "[DWH]\n",
    "DWH_CLUSTER_TYPE=multi-node\n",
    "DWH_NUM_NODES=4\n",
    "DWH_NODE_TYPE=dc2.large\n",
    "\n",
    "DWH_IAM_ROLE_NAME=dwhRole\n",
    "DWH_CLUSTER_IDENTIFIER=dwhCluster\n",
    "DWH_DB=dwh\n",
    "DWH_DB_USER=dwhuser\n",
    "DWH_DB_PASSWORD=<password>\n",
    "DWH_PORT=5439\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    Param                 Value\n",
       "0        DWH_CLUSTER_TYPE            multi-node\n",
       "1           DWH_NUM_NODES                     2\n",
       "2           DWH_NODE_TYPE             dc2.large\n",
       "3  DWH_CLUSTER_IDENTIFIER  msantoso4-dwhCluster\n",
       "4                  DWH_DB                   dwh\n",
       "5             DWH_DB_USER               dwhuser\n",
       "6         DWH_DB_PASSWORD                   XXX\n",
       "7                DWH_PORT                  5439\n",
       "8       DWH_IAM_ROLE_NAME  msantoso4-dwhRolePmi"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Param</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DWH_CLUSTER_TYPE</td>\n      <td>multi-node</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DWH_NUM_NODES</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DWH_NODE_TYPE</td>\n      <td>dc2.large</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DWH_CLUSTER_IDENTIFIER</td>\n      <td>msantoso4-dwhCluster</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DWH_DB</td>\n      <td>dwh</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>DWH_DB_USER</td>\n      <td>dwhuser</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>DWH_DB_PASSWORD</td>\n      <td>XXX</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>DWH_PORT</td>\n      <td>5439</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>DWH_IAM_ROLE_NAME</td>\n      <td>msantoso4-dwhRolePmi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh-iac.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "#(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \\\n",
    "                   \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \\\n",
    "                   \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER,\\\n",
    "                   DWH_DB, DWH_DB_USER, \"XXX\", DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create clients for EC2, S3, IAM, and Redshift\n",
    "Here, we will be using `boto3` to get the resource handles to talk to S3, EC2, IAM and Redshift. Notice that for Redshift, you will be creating a client and choose `ap-southeast-1` as region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "region = \"ap-southeast-1\"\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=region,\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=region,\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name=region\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=region,\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Step 4: Create an IAM role and assign it a policy so that it can read S3 bucket\n",
    "- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name msantoso4-dwhRolePmi already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::552702999317:role/msantoso4-dwhRolePmi\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:  Create a Redshift Cluster and Launch it\n",
    "\n",
    "- Create a RedShift Cluster\n",
    "- For complete arguments to `create_cluster`, see [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Step 6: *Describe* the cluster to see its status\n",
    "- Run this block several times until the cluster status becomes `Available`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  ClusterStatus       \n",
       "3  MasterUsername      \n",
       "4  DBName              \n",
       "5  Endpoint            \n",
       "6  VpcId               \n",
       "7  NumberOfNodes       \n",
       "\n",
       "                                                                                                  Value  \n",
       "0  msantoso4-dwhcluster                                                                                  \n",
       "1  dc2.large                                                                                             \n",
       "2  available                                                                                             \n",
       "3  dwhuser                                                                                               \n",
       "4  dwh                                                                                                   \n",
       "5  {'Address': 'msantoso4-dwhcluster.ce7s0wggntlb.ap-southeast-1.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6  vpc-76a35710                                                                                          \n",
       "7  2                                                                                                     "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Key</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ClusterIdentifier</td>\n      <td>msantoso4-dwhcluster</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NodeType</td>\n      <td>dc2.large</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ClusterStatus</td>\n      <td>available</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MasterUsername</td>\n      <td>dwhuser</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DBName</td>\n      <td>dwh</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Endpoint</td>\n      <td>{'Address': 'msantoso4-dwhcluster.ce7s0wggntlb.ap-southeast-1.redshift.amazonaws.com', 'Port': 5439}</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>VpcId</td>\n      <td>vpc-76a35710</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NumberOfNodes</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \\\n",
    "                  \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Make a note of <font color='red'> Cluster Endpoint and Role ARN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DWH_ROLE_ARN ::  arn:aws:iam::552702999317:role/msantoso4-dwhRolePmi\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## DO NOT RUN THIS unless the cluster status becomes \"Available\"\n",
    "##\n",
    "\n",
    "# DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "# #print(\"DWH_ENDPOINT :: \", endpoint)\n",
    "# print(\"DWN_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create Security group inbound rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "name 'myClusterProps' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Open an incoming  TCP port to access the cluster ednpoint\n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName= 'default',  \n",
    "        CidrIp='0.0.0.0/0',  \n",
    "        IpProtocol='TCP',  \n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Design your Data Pipeline.\n",
    "\n",
    "Our DAG here is called `s3-to-redshift`. \n",
    "\n",
    "This pipeline is generated using the below code. There are 3 essential components of a data pipeline:\n",
    "- **DAGs** : A DAG is made up of a sequence of operators.\n",
    "- **Operators**: An operator is basically a task (the rectangle box). The operator makes use of Hooks to connect with external entities like S3, Redshift etc.\n",
    "     * **PythonOperator**: This takes a callback function which defines the instructions that make up the task.\n",
    "     * **PostgresOperator**: This takes a sql argument, which when given a SQL statement, it executes it in the task.\n",
    "- **Hooks**: Think of a hook as getting the connection that you have defined in Airflow. (I describe this further down).\n",
    "\n",
    "The structure of the pipeline itself is very easy to layout. Like so:\n",
    "```\n",
    "create_table >> copy_task\n",
    "copy_task >> location_traffic_task\n",
    "```\n",
    "\n",
    "copy_task here is PythonOperator which takes `load_data_to_redshift` as the callback function.\n",
    "\n",
    "![data-pipeline](images/data-pipeline.png)\n",
    "\n",
    "### Code to generate the DAG\n",
    "\n",
    "```python\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "\n",
    "def load_data_to_redshift(*args, **kwargs):\n",
    "    # use AWS hook to get to aws connection id\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    \n",
    "    # use PostgresHook to get to redshift connection id\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    redshift_hook.run(sql_statements.COPY_ALL_TRIPS_SQL.format(credentials.access_key, credentials.secret_key))\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    's3-to-redshift-dag',\n",
    "    start_date=datetime.datetime.now()\n",
    ")\n",
    "\n",
    "create_table = PostgresOperator(\n",
    "    task_id=\"create_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "copy_task = PythonOperator(\n",
    "    task_id='load_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_data_to_redshift\n",
    ")\n",
    "\n",
    "location_traffic_task = PostgresOperator(\n",
    "    task_id=\"calculate_location_traffic\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.LOCATION_TRAFFIC_SQL\n",
    ")\n",
    "\n",
    "create_table >> copy_task\n",
    "copy_task >> location_traffic_task\n",
    "```\n",
    " \n",
    "\n",
    "### Callbacks and context variables\n",
    "It is worth mentioning here, that you can also pass context to the PythonOperator, which essentially allows it to access Airflow's runtime variables (context variables). \n",
    "\n",
    "```\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def hello_date(*args, **kwargs):\n",
    "    print(f“Hello {kwargs[‘execution_date’]}”)\n",
    "\n",
    "divvy_dag = DAG(...)\n",
    "task = PythonOperator(\n",
    "    task_id=’hello_date’,\n",
    "    python_callable=hello_date,\n",
    "    provide_context=True,\n",
    "    dag=divvy_dag)\n",
    "```\n",
    "All the context variables: \n",
    "https://airflow.apache.org/macros.html\n",
    "\n",
    "\n",
    "### Define S3 and Redshift connections in Airflow\n",
    "\n",
    "On the left is the `S3 connection`. The Login and password are the IAM user's access key and secret key that you created in part 1. Basically, by using these credentials, we are able to read data from S3.\n",
    "\n",
    "On the right is the `redshift connection`. These values can be easily gathered from your Redshift cluster in part 1.\n",
    "\n",
    "![connections](images/connections.png)\n",
    "\n",
    "### Define S3 variables for bucket and prefix (so that you don't have to hard code that in the code). Airflow maintains these for us. But we need to create it first.\n",
    "\n",
    "![variables](images/variables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Connect to cluster, load data and perform analysis\n",
    "\n",
    "In the second part, we will be making use of the cluster we just launched in part 1. \n",
    "\n",
    "You can quickly validate using the redshift console that the tables are created:\n",
    "\n",
    "![validate-on-redshift](images/validate-on-redshift.png)\n",
    "\n",
    "Alternately, you can connect to the redshift cluster from jupyter notebook, as shown below:\n",
    "\n",
    "Using sql extension, we can directly run SQL commands within jupyter notebook. \n",
    "- A single `%sql` means the query is a python string accessed using the dollar sign.\n",
    "- A `%%sql` means the query is not a python string but can be multiline SQL statements \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sql extension and connect to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'DWH_ENDPOINT' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b739293ae876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconn_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"postgresql://{}:{}@{}:{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDWH_DB_USER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDWH_DB_PASSWORD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDWH_ENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDWH_PORT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDWH_DB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sql'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'$conn_string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DWH_ENDPOINT' is not defined"
     ]
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.c4m4dhrmsdov.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "10 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>station_id</th>\n",
       "        <th>station_name</th>\n",
       "        <th>num_departures</th>\n",
       "        <th>num_arrivals</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>224</td>\n",
       "        <td>Halsted St &amp; Willow St</td>\n",
       "        <td>282</td>\n",
       "        <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>225</td>\n",
       "        <td>Halsted St &amp; Dickens Ave</td>\n",
       "        <td>419</td>\n",
       "        <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>59</td>\n",
       "        <td>Wabash Ave &amp; Roosevelt Rd</td>\n",
       "        <td>997</td>\n",
       "        <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>197</td>\n",
       "        <td>Michigan Ave &amp; Madison St</td>\n",
       "        <td>535</td>\n",
       "        <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>461</td>\n",
       "        <td>Broadway &amp; Ridge Ave</td>\n",
       "        <td>180</td>\n",
       "        <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>428</td>\n",
       "        <td>Dorchester Ave &amp; 63rd St</td>\n",
       "        <td>26</td>\n",
       "        <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>270</td>\n",
       "        <td>Stony Island Ave &amp; 75th St</td>\n",
       "        <td>5</td>\n",
       "        <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>367</td>\n",
       "        <td>Racine Ave &amp; 35th St</td>\n",
       "        <td>40</td>\n",
       "        <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>400</td>\n",
       "        <td>Cottage Grove Ave &amp; 71st St</td>\n",
       "        <td>6</td>\n",
       "        <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>639</td>\n",
       "        <td>Lakefront Trail &amp; Wilson Ave</td>\n",
       "        <td>19</td>\n",
       "        <td>27</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(224, 'Halsted St & Willow St', 282, 277),\n",
       " (225, 'Halsted St & Dickens Ave', 419, 490),\n",
       " (59, 'Wabash Ave & Roosevelt Rd', 997, 1099),\n",
       " (197, 'Michigan Ave & Madison St', 535, 571),\n",
       " (461, 'Broadway & Ridge Ave', 180, 225),\n",
       " (428, 'Dorchester Ave & 63rd St', 26, 26),\n",
       " (270, 'Stony Island Ave & 75th St', 5, 11),\n",
       " (367, 'Racine Ave & 35th St', 40, 33),\n",
       " (400, 'Cottage Grove Ave & 71st St', 6, 6),\n",
       " (639, 'Lakefront Trail & Wilson Ave', 19, 27)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM station_traffic limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Clean up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>DO NOT RUN THIS UNLESS YOU ARE SURE YOU WANT TO DELETE THE CLUSTER<br/> <b/>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "InvalidClusterStateFault",
     "evalue": "An error occurred (InvalidClusterState) when calling the DeleteCluster operation: There is an operation running on the Cluster. Please try to delete it at a later time.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidClusterStateFault\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1a51948b404c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### CAREFUL!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#-- Uncomment & run to delete the created resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mredshift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_cluster\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mClusterIdentifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDWH_CLUSTER_IDENTIFIER\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mSkipFinalClusterSnapshot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#### CAREFUL!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidClusterStateFault\u001b[0m: An error occurred (InvalidClusterState) when calling the DeleteCluster operation: There is an operation running on the Cluster. Please try to delete it at a later time."
     ]
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run this block several times until the cluster really deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ClusterNotFoundFault",
     "evalue": "An error occurred (ClusterNotFound) when calling the DescribeClusters operation: Cluster msantoso4-dwhcluster not found.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClusterNotFoundFault\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9b3202a2945e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyClusterProps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredshift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClusterIdentifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDWH_CLUSTER_IDENTIFIER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clusters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprettyRedshiftProps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyClusterProps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClusterNotFoundFault\u001b[0m: An error occurred (ClusterNotFound) when calling the DescribeClusters operation: Cluster msantoso4-dwhcluster not found."
     ]
    }
   ],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '718be433-7cce-4eb1-b926-f6fe2505ab27',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '718be433-7cce-4eb1-b926-f6fe2505ab27',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '200',\n",
       "   'date': 'Wed, 23 Dec 2020 08:54:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The power of infrastructure-as-code is evident in the fact that we were able to launch a 4-node Redshift cluster, perform our analysis, and destroy all the resources, without once having to login to AWS console. This is the essense of cloud computing, wherein, you can spin the resources as-and-when you want, do whatever task you wish to do, and clean up the resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}